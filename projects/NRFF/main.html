<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>NRFF</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-rbsA2VBKQhggwzxH7pPCaAqO46MgnOM80zW1RWuH61DGLwZJEdK2Kadq2F9CUG65" crossorigin="anonymous">

    <style>
      .container {
      width: auto;
      max-width: auto;
    }

    .iframe-container{
      position: relative;
      width: 100%;
      padding-bottom: 56.25%; 
      height: 0;
    }
    .iframe-container iframe{
      position: absolute;
      top:0;
      left: 0;
      width: 100%;
      height: 100%;
    }
    </style>
  </head>
  <body>
    <main>
      <div class="container">
        <h1 class="mt-5 text-center">Multiscale Tensor Decomposition and Rendering Equation Encoding for View Synthesis</h1>
        <h4 class="mt-4 text-center">CVPR 2023</h4>
        <div class="mt-4 text-center ">
                <a href="https://imkanghan.github.io/">Kang Han</a><sup>1</sup>, <a href="https://scholars.latrobe.edu.au/wxiang">Wei Xiang</a><sup>2</sup>
        </div>
        <div class="mt-4 text-center ">
            <sup>1</sup>James Cook University, <sup>2</sup>La Trobe University
        </div>
        <div class="mt-4 text-center ">
            <a href="https://arxiv.org/abs/2303.03808">Paper</a>, &nbsp; <a href="https://github.com/imkanghan/nrff">Code</a>
        </div>

        <div class="mt-4">
          <div class="iframe-container">
            <iframe width="560" height="315" src="https://www.youtube.com/embed/Sj095PTUUC8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
          </div>
        </div>

        <div class="mt-4">
            <h2>Abstract</h2>
            <p>Rendering novel views from captured multi-view images has made considerable progress since the emergence of the neural radiance field. This paper aims to further advance the quality of view synthesis by proposing a novel approach dubbed the neural radiance feature field (NRFF). We first propose a multiscale tensor decomposition scheme to organize learnable features so as to represent scenes from coarse to fine scales. We demonstrate many benefits of the proposed multiscale representation, including more accurate scene shape and appearance reconstruction, and faster convergence compared with the single-scale representation. Instead of encoding view directions to model view-dependent effects, we further propose to encode the rendering equation in the feature space by employing the anisotropic spherical Gaussian mixture predicted from the proposed multiscale representation. The proposed NRFF improves state-of-the-art rendering results by over 1 dB in PSNR on both the NeRF and NSVF synthetic datasets. A significant improvement has also been observed on the real-world Tanks & Temples dataset.</p>
        </div>

        <div class="mt-4">
          <h2>Multiscale Tensor Decomposition</h2>
          <div class="row">
            <div class="col-sm">
              <div class="d-flex justify-content-center">
                <img src='mtd.png' class='img-fluid' style="width:60%;max-width:600px;">
              </div>
              <p>The proposed multiscale tensor decomposition representation. At each level, a 3D tensor representation is decomposed to three sets of plane feature maps and line feature vectors. The resolution of decomposed tensors increases with the level, enabling scene representation at different scales. The concatenated feature vectors from all levels are used to predict parameters P (will be used to encode the rendering equation) by a spatial MLP.</p>
            </div>
            <div class="col-sm">
              <div class="d-flex justify-content-center">
                <img src='layers.png' class='img-fluid' style="width:80%;max-width:600px;">
              </div>
              <p>Performance comparison over training steps for varying the number of scale levels on scene ship. L indicates the number of levels. All models with different levels have roughly the same number of learnable features. Models with more levels not only converge faster but yield better final PSNRs.</p>
            </div>
          </div>
        </div>
          
        <div class="mt-4">
          <h2>Rendering Equation Encoding</h2>
          <div class="d-flex justify-content-center">
            <img src='ree.png' class='img-fluid' style="width:100%;max-width:1100px;">
          </div>
        </div>

        <div class="mt-4">
          <h2>Results</h2>
          <div class="d-flex justify-content-center">
            <img src='results.png' class='img-fluid' style="width:100%;max-width:1100px;">
          </div>
          <div class="d-flex justify-content-center">
            <img src='subjective.png' class='img-fluid' style="width:100%;max-width:1100px;">
          </div>
        </div>

        <div class="mt-4">
            <h2>Citation</h2>
            <p class="bg-light">
                @inproceedings{han2023nrff,
                    <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;title={Multiscale Tensor Decomposition and Rendering Equation Encoding for View Synthesis},
                    <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;author={Han, Kang and Xiang, Wei},
                    <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;booktitle={The IEEE / CVF Computer Vision and Pattern Recognition Conference},
                    <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pages={4232--4241},
                    <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;year={2023}
                <br>}
            </p>
        </div>
        
        
      <!-- </div> -->
    </main>
    
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-kenU1KFdBIe4zVF0s0G1M5b4hcpxyD9F7jL+jjXkk+Q2h455rYXK/7HAuoJl+0I4" crossorigin="anonymous"></script>
  </body>
</html>